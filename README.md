# PythonModel

# 项目依赖包

1. 导出所有项目的依赖包命令：pip3 freeze > requirements.txt
2. 安装项目依赖的时候使用命令：pip3 install -r requirements.txt

## jupyter notebook

1. 安装jupyter notebook：Terminal -> pip3 install jupyter notebook
2. 运行jupyter notebook：Terminal -> jupyter notebook

## 项目目录简介

0. 术语
    * 去均值：把输入数据各个维度都中心化为0，如下图所示，其目的就是把样本的中心拉回到坐标系原点上。
    * 归一化：幅度归一化到同样的范围，如下所示，即减少各维度数据取值范围的差异而带来的干扰。
      比如，我们有两个维度的特征A和B，A范围是0到10，而B范围是0到10000，如果直接使用这两个特征是有问题的，好的做法就是归一化，即A和B的数据都变为0到1的范围。
    * PCA/白化：用PCA降维；白化是对数据各个特征轴上的幅度归一化
1. deeplearn：深度学习
    * Keras 中文文档：https://keras.io/zh/
    * 安装TensorFlow：pip install tensorflow -i https://pypi.tuna.tsinghua.edu.cn/simple/
    * multilayerperception：多层感知
        * 向量修改模板：f(w ⃗ * X0 ⃗) ， ⃗ 代表向量表达式的箭头
        * 人工神经元的基本组成：输入数据、激励值计算、激活函数、Output
        * 激励值计算：A = ∑i=1-n (w[i] * x[i] + b)，写成向量形式为：Output = f(w⃗ * x⃗)。
          []代表角标，x[i]小写x代表输入X0(所有第一层输入数据)第i节点数据 A为下一层节点的边界函数，w[i]为各个input节点到下一层节点的权重，b为对应偏置值
        * 激活函数：Sigmoid函数(值域(0,1))、Tanh函数(f(x)= (e^x + e^(−x)) / (e^x − e^(−x))，值域(-1,1))、ReLU函数(线性修正单元激活函数)
        * 前向传播：第一层为输入层，输入输出都为X0(向量，或者说输入矩阵)。第二层为隐含层，输入X0，输出 X1 = f(w1⃗ * X0⃗)。第三层为输出层，输入X1，输出f(w2⃗ * X1⃗)
        * 反向传播：更新模型中的权重，我们需要从最后一层输出层开始，一层一层向前计算梯度、更新参数。我们需要求出损失函数对它们三者的偏导数，找到最优的w1 w2 w3使得误差最小。
        * 梯度消失和梯度爆炸(主要取决于激活函数的选择)：
            1. 梯度消失问题：当梯度较小时，可能会产生梯度消失问题。以sigmoid函数为例，sigmoid函数的导数最大值约为0.25。考虑一个5层的网络，传递到第一层时，梯度将会衰减为(0.25)^5 =
               0.0009，当小数过小超出表示精度时，计算机会将它作为机器零来处理，因此就会导致初始几层的参数基本不会更新。
            2. 梯度爆炸问题：当梯度较大时，可能会产生梯度爆炸问题。当输出层梯度大于1时，经过多层传递，很可能导致前几层的梯度非常巨大，每一次训练参数变化很大，使得模型训练困难，也很容易“走”出合理的区域。
            3. 解决方案是改进激活函数：
                1. 一种改进的激活函数就是前面介绍的ReLU函数。它的一大特点是未激活时梯度为0，激活后梯度恒为1，由于0和1在指数运算时的不变性，就可以有效地防止梯度消失和梯度爆炸问题。
                2. 梯度裁剪。简要地说就是设定一个阈值，如果求出来的梯度大于这个阈值，我们就将梯度强行缩减为等于阈值。这样也可以防止梯度爆炸问题。
    * convolutionalneuralnetwork：卷积神经网络
        * 特征检测器：卷积运算结果矩阵
        * 为什么使用卷积？和只用全连接层相比，卷积层的两个主要优势在于参数共享和稀疏连接，神经网络可以通过这两种机制减少参数，以便我们用更小的训练集来训练它，从而预防过度拟合。
            * 参数共享：特征检测如垂直边缘检测如果适用于图片的某个区域，那么它也可能适用于图片的其他区域。
              也就是说，如果你用一个3×3的过滤器检测垂直边缘，那么图片的左上角区域，以及旁边的各个区域（左边矩阵中蓝色方框标记的部分）都可以使用这个3×3的过滤器。
              每个特征检测器以及输出都可以在输入图片的不同区域中使用同样的参数，以便提取垂直边缘或其它特征。 它不仅适用于边缘特征这样的低阶特征，同样适用于高阶特征，例如提取脸上的眼睛，猫或者其他特征对象。
            * 稀疏连接：特征检测器矩阵中的0是通过卷积核(过滤器，比如：3×3)
              的卷积计算得到的，它只依赖于这个卷积核和当前3×3窗口的输入矩阵，特征检测器中的一个元素0仅与输入特征中9个相连接。而且矩阵中其它值都不会对输出的该0产生任何影响，这就是稀疏连接的概念。
        * 卷积神经网络的层级结构
            * 数据输入层/ Input layer：该层要做的处理主要是对原始图像数据进行预处理，其中包括：去均值、归一化、PCA/白化
            * 卷积计算层/ CONV layer：
                * 两个关键操作
                    * 局部关联。每个神经元看做一个滤波器(filter)
                    * 窗口(receptive field)滑动，filter对局部数据计算
                * 卷积层遇到的几个名词
                    * 步长/stride：窗口一次滑动的长度
                    * 深度depth(有时也叫通道channel)：有多少神经元(即多少个卷积核或者少过滤器，用于取多个特征)深度(通道)就是几。RGB三通道，在经过6个过滤器卷积后，通道为6
                    * 填充值/zero-padding：滑动窗口没法滑完，在原先矩阵加差的n层填充值0
                * 卷积运算：滤波器矩阵(或卷积核)和输入矩阵数据，通过滑动窗口的思想，分别对当前窗口各个矩阵位置数值乘积后想加，
                  成为当前窗口的卷积值，当滑动窗口移动完成一行时卷积运算卷积运算矩阵完成一行，知道窗口滑动结束，卷积运算特征矩阵已形成。
            * ReLU激励层 / ReLU layer：
                * 激励层的实践经验： ① 不要用sigmoid！不要用sigmoid！不要用sigmoid！ ② 首先试RELU，因为快，但要小心点 ③ 如果2失效，请用Leaky ReLU或者Maxout ④
                  某些情况下tanh倒是有不错的结果，但是很少
            * 池化层 / Pooling layer：
                * 池化层夹在连续的卷积层中间， 用于压缩数据和参数的量，减小过拟合。简而言之，如果输入是图像的话，那么池化层的最主要作用就是压缩图像。
                * 池化层的具体作用：
                    1. 特征不变性，也就是我们在图像处理中经常提到的特征的尺度不变性，池化操作就是图像的resize，
                       平时一张狗的图像被缩小了一倍我们还能认出这是一张狗的照片，这说明这张图像中仍保留着狗最重要的特征，我们一看就能判断图像中画的是一只狗，图像压缩时去掉的信息只是一些无关紧要的信息，而留下的信息则是具有尺度不变性的特征，是最能表达图像的特征。
                    2. 特征降维，我们知道一幅图像含有的信息是很大的，特征也很多，但是有些信息对于我们做图像任务时没有太多用途或者有重复，我们可以把这类冗余信息去除，把最重要的特征抽取出来，这也是池化操作的一大作用。
                    3. 在一定程度上防止过拟合，更方便优化。
                * 池化层用的方法：Max pooling 和 average pooling，而实际用的较多的是Max pooling。
            * 全连接层 / FC layer：两层之间所有神经元都有权重连接，通常全连接层在卷积神经网络尾部。也就是跟传统的神经网络神经元的连接方式是一样的
        * 卷积神经网络之优缺点
            * 优点：共享卷积核，对高维数据处理无压力、无需手动选取特征，训练好权重，即得特征分类效果好
            * 缺点：需要调参，需要大样本量，训练最好要GPU、物理含义不明确（也就说，我们并不知道没个卷积层到底提取到的是什么特征，而且神经网络本身就是一种难以解释的“黑箱模型”）
    * recurrentneuralnetwork：循环神经网络
2. machinelearn：机器学习
    * sklearn 中文文档：https://www.scikitlearn.com.cn/
    * linearregression：线性回归
        * 单因子线性回归 - 一元
        * 多因子线性回归实战 - 多元
    * logisticregression：逻辑回归(Sigmoid函数: f(x)=1/(1+e^−x))
        * 一阶边界函数： 𝜃0+𝜃1𝑋1+𝜃2𝑋2=0
        * 二阶边界函数： 𝜃0+𝜃1𝑋1+𝜃2𝑋2+𝜃3𝑋21+𝜃4𝑋22+𝜃5𝑋1𝑋2=0
    * clustering：聚类
        * K-means算法：K-平均或者K-均值
        * KNN，K-NearestNeighbor：邻近算法(监督学习)
        * meanShift算法：均值漂移，固定半径r
    * otheralgorithm：其它算法
        * Decision Tree：决策树算法
            * 信息熵(Entropy)：Ent(D)= −Σk=1~n (p(k) * log2p(k))
              D代表样本总量 k代表某一类别 p(k)代表某一类别在总样本类别中的比例
            * 某一属性信息增益(Information Gain)：Gain(D, a) = Ent(D) - Σv=1~n (Dv/D * Ent(Dv))
              a代表某一属性(特征) v代表属性a划分出来的类别数量,比如说a属性影响两种类别(总3种) Dv代表类别v样本数
        * Anomaly Detection：异常检测
            * 高斯分布：𝑓(𝑥) = (1/(√2𝜋 * 𝜎)) * (𝑒^(−((𝑥−𝜇)^2)/(2*𝜎^2)))，公式中μ为平均数，σ为标准差，f(x)为正态分布函数
            * 高阶高斯分布：p(x) = Πj=1~n 𝑓(j)，“Π”累乘符号 j代表对每一个维度都计算对应的正态分布
        * PCA(Principal Component Analysis)，即主成分分析方法：数据降维算法
3. modelevaluationoptimization：模型评价优化
    * 过拟合和欠拟合
        * 过拟合问题：训练数据准确，预测数据不准确
            * 原因：1. 模型结构过于复杂(维度过高)。 2. 使用了过多属性，模型训练时包含了干扰信息。
            * 解决办法：1. 简化模型结构(使用低阶模型，比如线性模型)。2. 数据预处理，保留主成分信息(数据PCA处理)。3. 在模型训练时，增加正则化项(regularization)
        * 欠拟合问题：训练数据不准确，预测数据也不准确
            * 解决办法：欠拟合可以通过观察训练数据及时发现问题，通过优化模型结果解决
    * 数据分离和混淆矩阵
        * 数据分离：1. 把数据分成两部分：训练家、测试集。2. 使用训练集数据进行模型训练。3. 使用测试集数据进行预测，更有效的评估模型对于新数据的预测表现
        * 混淆矩阵（confusion_matrix）
            * 分类评估指标中定义的一些符号含义:
                * TP(True Positive)：将正类预测为正类数，真实为0，预测也为0
                * FN(False Negative)：将正类预测为负类数，真实为0，预测为1
                * FP(False Positive)：将负类预测为正类数， 真实为1，预测为0
                * TN(True Negative)：将负类预测为负类数，真实为1，预测也为1
            * 概念：误差矩阵，用于衡量分类算法的准确程度
            * 背景场景：使用准确率进行模型评估的局限性：模型1和模型2表现的差别 源数据：1000个数据，900个1，100个0 模型1：850个1预测正确，50个个0预测准确，准确率为90%
              模型2：预测所有的样本结构都是1的准确率90% (空正确率)
            * 背景总结：准确率可以方便的用于衡量模型的整体预测效果，但无法反应细节信息，具体表现在：1. 没有体现数据预测的实际分布情况。2. 没有体现模型错误预测的类型
            * 混淆矩阵评价指标:
                * AccuracyRate(准确率): (TP+TN)/(TP+TN+FN+FP)
                * ErrorRate(误分率): (FN+FP)/(TP+TN+FN+FP)
                * Recall(召回率，查全率,击中概率): TP/(TP+FN), 在所有GroundTruth为正样本中有多少被识别为正样本了;
                * Precision(查准率):TP/(TP+FP),在所有识别成正样本中有多少是真正的正样本；
                * TPR(TruePositive Rate): TP/(TP+FN),实际就是Recall
                * FAR(FalseAcceptance Rate)或FPR(False Positive Rate)错误接收率/误报率：FP/(FP+TN)，在所有GroundTruth为负样本中有多少被识别为正样本了;
                * FRR(FalseRejection Rate)错误拒绝率/拒真率: FN/(TP+FN)，在所有GroundTruth为正样本中有多少被识别为负样本了，它等于1-Recall
    * 模型优化
        * 问题
            1. 问题一：用什么算法？
            2. 问题二：具体算法的核心结构或者参数如何选择？比如：1. 逻辑回归边界函数用线性还是多项式？2. KNN的核心参数n_neighbors取多少合适？
            3. 问题三：模型表现不佳，怎么办？
        * 如何提高模型表现？数据质量决定模型表现的上限
            1. 数据属性的意义，是否为无关数据
            2. 不同属性数据的数量级差异性如何。比如：身高1.8m和体重70kg
            3. 是否有异常数据
            4. 采集数据的方法是否合理，采集到的数据是否有代表性
            5. 对于标签结果，要确保标签判定规则的一致性(统一标准)
        * try --》 Benefits
            1. 删除不必要的属性 --》 减少过拟合、节约运算时间
            2. 数据预处理：归一化、标准化 --》 平衡数据影响，加快训练时间
            3. 确保是否保留或过滤异常数据 --》 提高鲁棒性
            4. 尝试不同的模型，对比模型表现 --》 帮助确定更合适的模型
        * 目标：在确定模型类别后，如何让模型表现更好。三个方面：数据、模型核心参数、正则化。尝试以下方法：
            1. 遍历核心参数组合，评估对应模型表现（比如：逻辑回归边界函数考虑多项式、KNN尝试不同的n_neighbors值(值越小，模型复杂度越高)）
            2. 扩大数据样本
            3. 增加或减少数据属性
            4. 对数据进行降维处理
            5. 对模型进行正则化处理，调整正则项λ的数值
        * 训练集数据准确率，随着模型复杂而越高。测试数据集准确率，在模型过于简单或者过于复杂的情况时下降。选择合适程度的模型
4. migrationhybridmodel：迁移混合模型
